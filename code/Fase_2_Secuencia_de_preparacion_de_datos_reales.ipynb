{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la primera casilla incluimos imports, rutas y funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import random\n",
    "import pyspark\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys,os #for data \n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "#file paths\n",
    "data_dir=\"D:\\\\Genomics\\\\1000_genomes\\\\release\\\\20130502\"\n",
    "chfile=\"ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\"\n",
    "#gen dictionary path\n",
    "genPosDicFile=\"D:\\\\Genomics\\\\genomePositionsDictionary.pkl\"\n",
    "#output path\n",
    "savedFileName=\"D:\\\\SparkFiles\\\\GenPositionMarkedChromosomeFiles\\\\\"+chfile\n",
    "\n",
    "#Load gen dict\n",
    "pkl_file = open(genPosDicFile, 'rb')\n",
    "GenomePosDict=pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "\n",
    "#aux functions\n",
    "def toCSVLine(data):\n",
    "    return ','.join(str(d) for d in data)\n",
    "\n",
    "def strToInt(x):\n",
    "    newList=[]\n",
    "    for value in x:\n",
    "        newList.append(int(float(value.replace('[', ''))))\n",
    "    return newList\n",
    "\n",
    "def strToIntRemoving2(x):\n",
    "    newList=[]\n",
    "    for value in x:\n",
    "        t=int(float(value.replace('[', '')))    \n",
    "        if t!=1 and t !=0 and newList!=[]:\n",
    "            t=1\n",
    "        newList.append(t)   \n",
    "    return newList\n",
    "\n",
    "#funciones auxiliares: parsear archivos VCF\n",
    "def parserChrVariation(x):\n",
    "    newList=[]\n",
    "    for value in x:\n",
    "        if value=='0|0':\n",
    "            newList.append(0)\n",
    "        elif value=='1|1':\n",
    "            newList.append(2)\n",
    "        else:  \n",
    "            newList.append(1)\n",
    "    return newList\n",
    "\n",
    "def chrtointfrom0(chr):\n",
    "    if chr[0]=='X': return 23 # 22 +x+y\n",
    "    elif chr[0]=='Y': return 24\n",
    "    elif chr[0]=='M': return 25   #mitocondrial ?\n",
    "    elif chr[0]=='U': return 26   # ???\n",
    "    else: return int(chr[0:])\n",
    "genmultiplier=1000000000000    \n",
    "    \n",
    "#Mapping with BinarySearch to gen dict         \n",
    "def mapgenomeBinarySearch(dictionary, value):\n",
    "    if dictionary.get(value,None)!=None:\n",
    "        return dictionary.get(value)\n",
    "    vmin=0\n",
    "    vmax=len(dictionary.keys())-1 \n",
    "    pivot=0\n",
    "    keyslist=list(dictionary.keys())\n",
    "    while True:\n",
    "        pivot1=(vmax+vmin)/2\n",
    "        pivot2=(vmax+vmin)//2\n",
    "        if(pivot1==pivot2):\n",
    "            pivot=pivot2\n",
    "        else:\n",
    "            pivot=pivot2+1\n",
    "        if(vmax-vmin)<=1:\n",
    "            return dictionary.get(keyslist[pivot])\n",
    "        if keyslist[pivot]<value:\n",
    "            vmin=pivot\n",
    "        else:\n",
    "            vmax=pivot\n",
    "            \n",
    "#test value generation\n",
    "probabilityTuple=(0.7,0.4,0.95,0.02,0.4)\n",
    "\n",
    "def Rdd_to_Coordinate_Rdd(rddName):\n",
    "    return (rddName.zipWithIndex()\n",
    "          .flatMap(lambda x: ([(x[1],j[0],float(j[1]))for j in enumerate(x[0])]))\n",
    "           ) \n",
    "\n",
    "def getExtraCoordinateValues(probabilityTuple,numberAlteredPoints):\n",
    "    TempList=[]\n",
    "    for n in range(0,numberAlteredPoints):\n",
    "        T1=list(map(lambda x:x*random.random(),(1,1,1,1)))\n",
    "        T1[0]=int(T1[0]<0.05)#prob de esta alteracion\n",
    "        T1[1]=int(T1[1]<0.05)#prob de esta alteracion\n",
    "    \n",
    "        if T1[0]==1 and T1[1]==1:\n",
    "            T1[3]=int(T1[3]<probabilityTuple[2])\n",
    "        elif T1[0]==1:\n",
    "            T1[3]=int(T1[3]<probabilityTuple[0])\n",
    "        elif T1[1]==1: #T2==1 true\n",
    "            T1[3]=int(T1[3]<probabilityTuple[1])\n",
    "        else:\n",
    "            T1[3]=int(T1[3]<probabilityTuple[3])\n",
    "        if T1[2]<probabilityTuple[4]: \n",
    "            T1[3]=0  #low age limit \n",
    "        #modificacion para usar enteros\n",
    "            T1[2]=0\n",
    "        else:\n",
    "            T1[2]=1\n",
    "        TempList.append(T1)\n",
    "    return TempList  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente casilla contamos con la carga de archivos VCF,su parseo, filtrado y mapeado, guardando el resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header_row_number=252\n",
    "realDataRDD=(sc.textFile(data_dir+'\\\\'+chr14_file)\n",
    "             .zipWithIndex()\n",
    "             .filter(lambda x: x[1]>header_row_number))\n",
    "\n",
    "parsedRDD=(realDataRDD.map(lambda x: x[0].split(\"\\t\"))\n",
    "           .map(lambda x: [chrtointfrom0(x[0])*genmultiplier+int(x[1])]+parserChrVariation(x[9:]))\n",
    "           .map(lambda x: (mapgenome(GenomePosDict,x[0]),x[1:]) )\n",
    "           .filter(lambda x: x[0]!=[])\n",
    "           .flatMap(lambda x: (map(lambda i: (i,x[1]),x[0])))\n",
    "           .map(lambda x: [x[0]]+x[1])\n",
    "           )\n",
    "\n",
    "parsedRDD.map(toCSVLine).saveAsTextFile(savedFileName,compressionCodecClass=\"org.apache.hadoop.io.compress.GzipCodec\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente casilla carga los datos generados anteriormente, realizando fase de reduccion y guardando el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "chfile=\"ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\"\n",
    "inputSavedFileName=\"D:\\\\SparkFiles\\\\GenPositionMarkedChromosomeFiles\\\\\"+chfile\n",
    "outpuSsavedFileName=\"D:\\\\SparkFiles\\\\GenPositionMarkedChromosomeFiles\\\\ReducedByGen\\\\\"+chfile\n",
    "\n",
    "baseRDD=(sc.textFile(inputSavedFileName)\n",
    "        .map(lambda x: strToInt(x.split(\",\")))\n",
    "        .map(lambda x: (x[0],np.asarray(x[1:]))) \n",
    "        .reduceByKey(lambda x,y: np.logical_or(x,y))\n",
    "        .map(lambda x:(x[0],x[1].astype(int)))\n",
    "        .map(lambda x: [x[0]]+x[1].tolist()) \n",
    "        )\n",
    "\n",
    "baseRDD.map(toCSVLine).saveAsTextFile(outputSavedFileName,compressionCodecClass=\"org.apache.hadoop.io.compress.GzipCodec\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación,se cargan los archivos deseados, preparados anteriormente, para combinarlos en un solo RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sub paths\n",
    "file_ending=\".phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\"\n",
    "file_start=\"D:\\\\SparkFiles\\\\GenPositionMarkedChromosomeFiles\\\\ReducedByGen\\\\\"+\"ALL.chr\"\n",
    "#filename range (genome number name)\n",
    "gen_range=(10,12)\n",
    "\n",
    "\n",
    "\n",
    "cleanedRDD=None\n",
    "for item in range(gen_range[0],gen_range[1]):\n",
    "    if cleanedRDD==None:\n",
    "        cleanedRDD=(sc.textFile(file_start+str(item)+file_ending)\n",
    "            .map(lambda x: strToIntRemoving2(x.split(\",\")))\n",
    "            )\n",
    "    else:\n",
    "        cleanedRDD=cleanedRDD.union(\n",
    "            (sc.textFile(file_start+str(item)+file_ending)\n",
    "            .map(lambda x: strToIntRemoving2(x.split(\",\")))\n",
    "            )       \n",
    "        )\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos en esta sección realizaramos la transposición del RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transposedRDD=(\n",
    "    Rdd_to_Coordinate_Rdd(cleanedRDD)\n",
    "    .map(lambda x: (x[1],[(x[0],x[2])]))\n",
    "    .reduceByKey(lambda x,y: x+y)\n",
    "    .map(lambda x: [x[0]] + [y[1] for y in sorted(x[1])])   \n",
    "    .cache()\n",
    "    )\n",
    "\n",
    "#name for specific file\n",
    "name=\"10to11\"\n",
    "#store transposed rdd\n",
    "transposedRDD.map(toCSVLine).saveAsTextFile(file_start+name+\"transposed\",compressionCodecClass=\"org.apache.hadoop.io.compress.GzipCodec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente realizamos las ultimas conversiones de datos y almacenamos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##name for specific file\n",
    "name=\"10to11\"\n",
    "\n",
    "#index creation and appending generated values to the rdd\n",
    "indexes=transposedRDD.filter(lambda x: x[0]==0).map(lambda x: x[1:]+[0,1,2,3]).take(1)[0]\n",
    "indexeslen=len(indexes)\n",
    "labeledPointsRDD=(transposedRDD\n",
    "                  .filter(lambda x: x[0]!=0)\n",
    "                  .map(lambda x: x[1:]+[0,1,2,3] if x[0]== 0 else x[1:]+getExtraCoordinateValues(probabilityTuple,1)[0])                 \n",
    "                  .map(lambda x: LabeledPoint(x[-1],x))\n",
    "                 )\n",
    "#store generated files\n",
    "filename=file_start+name+\"_0.05P_Alteraciones\"\n",
    "pyspark.mllib.util.MLUtils.saveAsLibSVMFile(labeledPointsRDD,filename+\"Prepared_LabeledPoints\")\n",
    "transposedRDD.filter(lambda x: x[0]==0).map(lambda x: x+[0,1,2,3]).map(toCSVLine).saveAsTextFile(filename+\"Prepared_gen_positions\",compressionCodecClass=\"org.apache.hadoop.io.compress.GzipCodec\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
